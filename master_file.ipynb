{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Meteorite_Landings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Details about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc12894-5950-4167-bdbe-8b39abc8f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column Names:\", df.columns.tolist())\n",
    "rows, cols = df.shape\n",
    "\n",
    "#Before Cleaning\n",
    "print(f\"The DataFrame has {rows} rows and {cols} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53190bb4-f318-47c0-868c-dbcd01c847da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import io\n",
    "\n",
    "io.renderers.default = 'iframe'\n",
    "px.bar(data_frame=df['recclass'].value_counts().to_frame().reset_index().head(n=40), x='recclass', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Unnamed: 10' column, seems all the row's are missing values\n",
    "df.drop(columns=['Unnamed: 10'], inplace=True)\n",
    "\n",
    "# Handling missing values\n",
    "df = df.dropna(subset=['mass (g)', 'year', 'reclat', 'reclong'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning\n",
    "af_rows, af_cols = df.shape\n",
    "\n",
    "# DataFrame shape before and after cleaning\n",
    "print(\"DataFrame Shape Summary\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Before Cleaning : {rows:,} rows Ã— {cols:,} columns\")\n",
    "print(f\"After Cleaning  : {af_rows:,} rows Ã— {af_cols:,} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad4beb-aafa-40ab-a6e7-198b2508f07f",
   "metadata": {},
   "source": [
    "### Convert 'year' to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'year' to datetime\n",
    "print(f\"Before Conversion: {df['year'].dtype}\")\n",
    "\n",
    "# Convert 'year' to datetime\n",
    "df['year'] = pd.to_datetime(df['year'], format='%Y', errors='coerce')\n",
    "print(f\"After Conversion : {df['year'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a49d2-09cd-4b88-a3c1-5774a59d1a59",
   "metadata": {},
   "source": [
    "### Convert 'fall' column to binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123a0e6-21c3-4352-a56f-97db2c7e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fall'] = df['fall'].str.strip().str.capitalize()\n",
    "print(df['fall'].unique())\n",
    "\n",
    "df['fall'] = df['fall'].map({'Fell': 1, 'Found': 0})\n",
    "#checking values correctness after conversion\n",
    "print(df['fall'])\n",
    "print(f\"After Conversion : {df['fall'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cleaned data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['nametype'].value_counts())\n",
    "print(\"-\" * 30)\n",
    "print(df['fall'].value_counts())  # 'Fell' vs 'Found'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Fell vs Found Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot for fall status\n",
    "sns.countplot(x='fall', data=df)\n",
    "plt.title(\"Fell vs Found Meteorites\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Histogram for mass (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.log1p(df['mass (g)']), bins=50, kde=True, color='blue')\n",
    "plt.xlabel('Log Mass (g)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Meteorite Mass (Log Scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive map\n",
    "fig = px.scatter_geo(df,\n",
    "                     lon='reclong',\n",
    "                     lat='reclat',\n",
    "                     color='fall',\n",
    "                     hover_name='name',\n",
    "                     hover_data=['mass (g)', 'recclass'],\n",
    "                     projection='natural earth',\n",
    "                     title='Meteorite Falls vs Found Map')\n",
    "\n",
    "# Update layout\n",
    "fig.update_geos(showcountries=True, showland=True, landcolor=\"lightgray\", showocean=True, oceancolor=\"lightblue\")\n",
    "fig.update_layout(height=600, margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0})\n",
    "\n",
    "# Show the map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation between numeric features\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'year' to datetime and extract year as integer\n",
    "df['year'] = pd.to_datetime(df['year'], errors='coerce')\n",
    "df['year_int'] = df['year'].dt.year\n",
    "\n",
    "# Filter using the integer year\n",
    "df_recent = df[(df['year_int'] > 1970) & (df['year_int'] < 2025)]\n",
    "\n",
    "# Now plot with Plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    data_frame=df_recent,\n",
    "    lat='reclat',\n",
    "    lon='reclong',\n",
    "    color='year_int',              # or use 'year' if you prefer a datetime color scale\n",
    "    size='mass (g)',\n",
    "    size_max=15,\n",
    "    hover_name='name',\n",
    "    hover_data={\n",
    "        'recclass': True,\n",
    "        'mass (g)': True,\n",
    "        'year_int': True\n",
    "    },\n",
    "    mapbox_style='carto-darkmatter',\n",
    "    zoom=1,\n",
    "    center={'lat': 0, 'lon': 0},\n",
    "    title=\"Meteorite Falls (1970 - 2024) Visualized on Map\",\n",
    "    height=800,\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "falls_over_time = df.groupby(df['year'].dt.year).size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(falls_over_time.index, falls_over_time.values, marker='o')\n",
    "plt.title('Number of Meteorite Falls Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Meteorites')\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='fall', y='mass (g)', data=df)\n",
    "plt.title('Mass of Meteorites by Fall Type')\n",
    "plt.xlabel('Fall Type')\n",
    "plt.ylabel('Mass in grams')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc0623-6a60-4e74-87fc-8ad50daeab48",
   "metadata": {},
   "source": [
    "## Question 1:What is the distribution of meteorite classifications (recclass) and how does it differ between observed falls (Fell) and discovered finds (Found)? (descriptive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c77efb-6042-42a0-a607-f1ab6b46826d",
   "metadata": {},
   "source": [
    "### Preprocess for the visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad6081-89ac-45aa-a453-7a40e3ffe2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum total occurrences to be included in a class\n",
    "min_count = 10  \n",
    "\n",
    "recclass_fall_counts = df.groupby(['recclass', 'fall']).size().reset_index(name='count')\n",
    "recclass_pivot = recclass_fall_counts.pivot(index='recclass', columns='fall', values='count').fillna(0)\n",
    "recclass_pivot.columns = ['Found', 'Fell']\n",
    "recclass_pivot['total'] = recclass_pivot.sum(axis=1)\n",
    "\n",
    "# Filter out classes less than min_count as the rare classes\n",
    "recclass_pivot = recclass_pivot[recclass_pivot['total'] >= min_count].sort_values('total', ascending=False)\n",
    "\n",
    "# Add a normalized metric\n",
    "recclass_pivot['fell_ratio'] = recclass_pivot['Fell'] / recclass_pivot['total']  \n",
    "recclass_pivot['found_ratio'] = 1 - recclass_pivot['fell_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01cb01-b75e-4ed0-ba3e-6759a8f82e05",
   "metadata": {},
   "source": [
    "### Visualize the top classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b93b77-b248-4fcf-b271-99131701bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual-axis plot: counts + ratios\n",
    "fig, ax1 = plt.subplots(figsize=(14,7))\n",
    "ax2 = ax1.twinx()\n",
    "top20 = recclass_pivot.head(20)\n",
    "# Absolute counts (stacked bars)\n",
    "bars = top20[['Fell','Found']].plot.bar(stacked=True, ax=ax1, color=['#ff6b6b','#4ecdc4'], label=['Fell', 'Found'])\n",
    "ax1.set_ylabel('Total Count', color='black')\n",
    "\n",
    "# Fall ratio (line plot)\n",
    "line, = ax2.plot(top20['fell_ratio'], marker='o', color='#ff0000', label='Fell Ratio (Fell/Total)')\n",
    "ax2.set_ylabel('Fell Ratio (Fell/Total)', color='#ff0000')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines_labels = ax1.get_legend_handles_labels()\n",
    "line2_labels = ([line], [line.get_label()])\n",
    "handles = lines_labels[0] + line2_labels[0]\n",
    "labels = lines_labels[1] + line2_labels[1]\n",
    "\n",
    "# Set legend position\n",
    "ax1.legend(handles, labels, loc='upper left', bbox_to_anchor=(0, 1.15), ncol=3, fontsize=10)\n",
    "\n",
    "plt.title('Top Meteorite Classes: Counts vs Fall Ratios', fontsize=16)\n",
    "ax1.tick_params(axis='x', rotation=80)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cea3e-a11d-47f8-94de-19d35a1eb197",
   "metadata": {},
   "source": [
    "### Showing the classes with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4e9d3-d3c7-49f5-b933-55e6a907cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the biased classes\n",
    "\n",
    "print(\"\\nClasses Most Biased Toward Falls:\")\n",
    "display(recclass_pivot.sort_values('fell_ratio', ascending=False).head(5)[['Fell','total','fell_ratio']])\n",
    "\n",
    "print(\"\\nClasses Most Biased Toward Finds:\")\n",
    "display(recclass_pivot.sort_values('found_ratio', ascending=False).head(5)[['Found','total','found_ratio']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba9651-363f-4416-9c1a-677840be35de",
   "metadata": {},
   "source": [
    "### Interactive treemap for showing every recclass in 1 go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f035e-c618-4449-8fa5-585acb0eeef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treemap for rare classes (complementary insight)\n",
    "\n",
    "rare_classes = recclass_fall_counts[~recclass_fall_counts['recclass'].isin(top20.index)]\n",
    "px.treemap(rare_classes, path=['recclass'], values='count', \n",
    "           title='Rare Meteorite Classifications (Total < 10)')\n",
    "\n",
    "#ps: i kinda like how dumb this look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c733997-aaf7-46d7-b27b-ebf97cc02629",
   "metadata": {},
   "source": [
    "## Question 2: Investigate whether meteorietes with a higher mass (grams) exhibit distinct classifications patterns or geographic clustering compared to metorites with lower mass (Exploratory)\n",
    "\n",
    "explore relationships between:\n",
    "- mass\n",
    "- classfication (recclass)\n",
    "- geographic location (reclat, reclong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1638d6-1103-4cb7-be43-84de9240e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Group by recclass\n",
    "class_stats = df.groupby('recclass')['mass (g)'].agg(['count', 'mean', 'median', 'max']).sort_values(by='mean', ascending=False)\n",
    "\n",
    "# Plot top 10 classes by average mass\n",
    "class_stats.head(10).plot(kind='bar', y='mean', title='Top 10 Meteorite Classes by Average Mass')\n",
    "plt.ylabel('Average Mass (g)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot: Mass distribution per classification\n",
    "top_classes = df['recclass'].value_counts().head(10).index\n",
    "sns.boxplot(x='recclass', y='mass (g)', data=df[df['recclass'].isin(top_classes)])\n",
    "plt.xticks(rotation=45)\n",
    "plt.yscale('log')  # because mass likely has a long tail\n",
    "plt.title('Mass Distribution by Recclass')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2b66e-8add-42e8-af0a-3c3efb61932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Filter for valid geolocations and mass\n",
    "# df_geo = df[df['reclat'].notna() & df['reclong'].notna() & df['mass (g)'].notna()]\n",
    "\n",
    "# # Scatter plot on map\n",
    "# # world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "# url = \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
    "# world = gpd.read_file(url)\n",
    "# fig, ax = plt.subplots(figsize=(15, 10))\n",
    "# world.plot(ax=ax, color='lightgray')\n",
    "\n",
    "# # Normalize mass for better visualization\n",
    "# sizes = (df_geo['mass (g)'] / df_geo['mass (g)'].max()) * 100\n",
    "\n",
    "# plt.scatter(df_geo['reclong'], df_geo['reclat'], s=sizes, c=df_geo['mass (g)'], cmap='viridis', alpha=0.5)\n",
    "# plt.colorbar(label='Mass (g)')\n",
    "# plt.title('Meteorite Landings: Mass and Geographic Distribution')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306be78-8632-45f7-821c-65fa7d847575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mass_bins = [0, 1000, 10000, 50000, df['mass (g)'].max()]\n",
    "bin_labels = ['Small', 'Medium', 'Large', 'Very Large']\n",
    "\n",
    "# Im creating a new column here based on the bins\n",
    "df['mass_category'] = pd.cut(df['mass (g)'], bins=mass_bins, labels=bin_labels, include_lowest=True) \n",
    "#df['mass_category'].head(20) #checking values\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    df,\n",
    "    lat='reclat',\n",
    "    lon='reclong',\n",
    "    color='mass_category', \n",
    "    size='mass (g)',\n",
    "    size_max=20,\n",
    "    mapbox_style='carto-positron',\n",
    "    title='Meteorite Locations by Mass Category',\n",
    "    zoom=1,\n",
    "    height=700,\n",
    "    hover_name='name',\n",
    "    hover_data=['mass (g)', 'recclass'],\n",
    "    category_orders={\n",
    "        'mass_category': ['Small', 'Medium', 'Large', 'Very Large']  \n",
    "    }\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5a0d6-6085-47e6-b447-573d921b66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_classes = df['recclass'].value_counts().nlargest(10).index\n",
    "df_top = df[df['recclass'].isin(top_classes)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df_top, x='recclass', hue='mass_category')  # or mass_quartile\n",
    "plt.title('Meteorite Classification by Mass Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735561e-12d1-4c5b-b0a1-c9995caa70ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 4: How accurately can classification models predict whether a meteorite fell or was found, using its mass, classification type, and geographic coordinates as input features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366dec7-bce8-4bc8-9435-aae3c61428d3",
   "metadata": {},
   "source": [
    "### Predicting Meteorite Origin: Classification Models\n",
    "\n",
    "#### Objective\n",
    "**Question**: How accurately can classification models predict whether a meteorite **fell** or was **found**, using its:\n",
    "- `mass (g)`\n",
    "- `classification type` (`recclass`)\n",
    "- `geographic coordinates` (`reclat`, `reclong`)\n",
    "\n",
    "The goal is to apply and compare different classification algorithms to determine which model best predicts the fall status (`fall` column) of meteorites.\n",
    "\n",
    "We will explore the following models:\n",
    "1. **Logistic Regression**\n",
    "2. **Random Forest Classifier**\n",
    "3. **Support Vector Machine (SVM)**\n",
    "\n",
    "Each model will be evaluated using:\n",
    "- Confusion Matrix\n",
    "- Classification Report (Precision, Recall, F1-score)\n",
    "- Visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc86a0-62e2-40a5-9120-5850f9b284d4",
   "metadata": {},
   "source": [
    "### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216ff64-4d48-4719-87ff-802fd7ff3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df[['mass (g)', 'recclass', 'reclat', 'reclong', 'fall']].dropna()\n",
    "\n",
    "# Encode categorical feature\n",
    "df['recclass'] = LabelEncoder().fit_transform(df['recclass'])\n",
    "\n",
    "# Features and target\n",
    "X = df[['mass (g)', 'recclass', 'reclat', 'reclong']]\n",
    "y = df['fall']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821dbc69-6a3d-43ee-bbdb-38fc4975e134",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c5173-ab25-4d2e-9cbb-72564dc9e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Save classification report\n",
    "lr_report = classification_report(y_test, lr_pred, target_names=[\"Found\", \"Fell\"], output_dict=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_pred)\n",
    "lr_cm_df = pd.DataFrame(lr_cm, index=['Actual: Found (0)', 'Actual: Fell (1)'],\n",
    "                                   columns=['Predicted: Found (0)', 'Predicted: Fell (1)'])\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ“˜ Logistic Regression Confusion Matrix:\")\n",
    "print(lr_cm_df)\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test, lr_pred, target_names=[\"Found\", \"Fell\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e2508-70e0-4fd1-9853-250b0e7fa991",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf85b5b-39c8-470b-9f20-ff9faa6338ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Save classification report for later comparison\n",
    "rf_report = classification_report(y_test, rf_pred, target_names=[\"Found\", \"Fell\"], output_dict=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_pred)\n",
    "rf_cm_df = pd.DataFrame(rf_cm, index=['Actual: Found (0)', 'Actual: Fell (1)'],\n",
    "                                  columns=['Predicted: Found (0)', 'Predicted: Fell (1)'])\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸŒ³ Random Forest Confusion Matrix:\")\n",
    "print(rf_cm_df)\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred, target_names=[\"Found\", \"Fell\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176169dc-5157-4a49-972a-a912d025ed43",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaab8f7-6cde-4fa5-95f5-5277d0ee42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Save classification report\n",
    "svm_report = classification_report(y_test, svm_pred, target_names=[\"Found\", \"Fell\"], output_dict=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "svm_cm = confusion_matrix(y_test, svm_pred)\n",
    "svm_cm_df = pd.DataFrame(svm_cm, index=['Actual: Found (0)', 'Actual: Fell (1)'],\n",
    "                                     columns=['Predicted: Found (0)', 'Predicted: Fell (1)'])\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ”² SVM Confusion Matrix:\")\n",
    "print(svm_cm_df)\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred, target_names=[\"Found\", \"Fell\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc145c-c5cd-4c25-8564-f05636a236ea",
   "metadata": {},
   "source": [
    "### Visual Comparison of Confusion Matrices\n",
    "\n",
    "Below are the confusion matrices for the three models. Logistic Regression, Random Forest, and SVM. These matrices visually represent how well each model performed in classifying the meteorites as \"Found\" or \"Fell\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2637c60-0d3f-4385-8fe2-4b5a173d6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Logistic Regression\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title(\"Logistic Regression\")\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "\n",
    "# Random Forest\n",
    "sns.heatmap(rf_cm_df, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title(\"Random Forest\")\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "# SVM\n",
    "sns.heatmap(svm_cm_df, annot=True, fmt='d', cmap='Purples', ax=axes[2])\n",
    "axes[2].set_title(\"SVM\")\n",
    "axes[2].set_xlabel(\"Predicted\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c1239-38c0-4b96-a402-381cc19ecc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"SVM\"],\n",
    "    \"Accuracy\": [lr_report['accuracy'], rf_report['accuracy'], svm_report['accuracy']],\n",
    "    \"Precision (Fell)\": [lr_report['Fell']['precision'], rf_report['Fell']['precision'], svm_report['Fell']['precision']],\n",
    "    \"Recall (Fell)\": [lr_report['Fell']['recall'], rf_report['Fell']['recall'], svm_report['Fell']['recall']],\n",
    "    \"F1-Score (Fell)\": [lr_report['Fell']['f1-score'], rf_report['Fell']['f1-score'], svm_report['Fell']['f1-score']]\n",
    "})\n",
    "\n",
    "comparison_df.set_index(\"Model\", inplace=True)\n",
    "comparison_df = comparison_df.round(3)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d99fc-db5b-45a4-8179-2dfd41edb31b",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "\n",
    "- **Logistic Regression** shows high overall accuracy but fails to capture \"Fell\" cases â€” very low recall and F1-score.\n",
    "- **SVM** improves on logistic regression but still underperforms in recall for the \"Fell\" class.\n",
    "- **Random Forest** clearly outperforms the others in all metrics related to the minority class (\"Fell\").\n",
    "\n",
    "---\n",
    "\n",
    "### Final Verdict:\n",
    "**Random Forest Classifier** is the best model for this classification task. It balances precision and recall effectively, especially for the underrepresented class, and delivers the most robust results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a38875-ecd5-469e-9a59-883a7722ffe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
